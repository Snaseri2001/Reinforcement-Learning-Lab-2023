{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c52de556",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Wrapper\n",
    "import numpy as np\n",
    "import math\n",
    "import pygame\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5686ccff",
   "metadata": {},
   "source": [
    "# Policy Iteration \n",
    "here we are going to implement Policy Iteration in Forzen_Lake (one of the environmnet in OpenAi Gym library .\n",
    "![title](Policy_itr.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5805b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(P, nS, nA,env ,  gamma=0.9, tol=1e-4):\n",
    "    '''\n",
    "    parameters:\n",
    "        P: transition probability matrix\n",
    "        nS: number of states\n",
    "        nA: number of actions\n",
    "        gamma: discount factor\n",
    "        tol: tolerance for convergence\n",
    "    returns:\n",
    "        value_function: value function for each state\n",
    "        policy: policy for each state\n",
    "    '''\n",
    "    # initialize value function and policy\n",
    "    \n",
    "    value_function = np.zeros(nS)\n",
    "    Policy = []\n",
    "    Possible_actions = [0 , 1 , 2 , 3]\n",
    "    '''\n",
    "    action 0 : Move to the right\n",
    "    acrion 1 : Move to the left\n",
    "    action 2 : Move up\n",
    "    actino 3 : Move down\n",
    "\n",
    "    '''\n",
    "    for i in range(nS) :\n",
    "#         in this loop we add an action list to Policy list , we cinsider one possible_action list for each state\n",
    "        Policy.append(Possible_actions)\n",
    "    stable_policy = True\n",
    "    counter = 0 \n",
    "#     while stable_policy == True :\n",
    "    for i in range(1) :\n",
    "        value_function = Policy_evaluation(Policy , env , nS  , gamma , value_function, theta = 0.01 )  \n",
    "        q = value_function\n",
    "        stable_policy , Policy = Policy_Improvemnet(Policy , env , nS  , gamma , q, teta = 0.01)\n",
    "        \n",
    "    # Implement policy iteration here #\n",
    "    return value_function, Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a7cb952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Policy_evaluation(policy, env, nS, gamma, value_function, theta=0.01):\n",
    "    counter = 0\n",
    "\n",
    "    # Loop until the change in value is less than the threshold value\n",
    "    while True:\n",
    "        delta = -math.inf\n",
    "        values = np.zeros(nS)\n",
    "\n",
    "        # Loop through each state\n",
    "        for state in range(nS):\n",
    "            # Check if the state is a terminal state\n",
    "            if state == 5 or state == 7 or state == 11 or state == 12:\n",
    "                values[state] = 0\n",
    "            # Check if the state is the final state\n",
    "            elif state == 15:\n",
    "                values[state] = 0\n",
    "            else:\n",
    "                # Initialize the new_value and number_of_actions variables\n",
    "                new_value = 0\n",
    "                number_of_actions = len(policy[state])\n",
    "                for action in policy[state]:\n",
    "                    # Loop through each possible outcome for the current action in the current state\n",
    "                    for a in env.env.P[state][action]:\n",
    "                        reward = 0\n",
    "                        # Calculate the reward based on the outcome\n",
    "                        if a[3] == True and a[1] == 15:\n",
    "                            reward = 1\n",
    "                        elif a[3] == True:\n",
    "                            reward = -2\n",
    "                        elif a[3] == False:\n",
    "                            reward = 0\n",
    "                        # Calculate the expected value for the current action in the current state\n",
    "                        new_value += a[0] * (reward + gamma * value_function[a[1]]) / number_of_actions\n",
    "                # Update the value for the current state\n",
    "                values[state] = format(new_value, \".3f\")\n",
    "                # Update the delta variable\n",
    "                delta = max(delta, np.abs(value_function[state] - values[state]))\n",
    "        # Update the value function\n",
    "        value_function = values\n",
    "        # Increment the counter variable\n",
    "        counter += 1\n",
    "        if delta < theta:\n",
    "            break\n",
    "    # Return the updated value function\n",
    "    return value_function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbd0d566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Policy_Improvemnet(Policy, env, nS, gamma, value_function, teta=0.01):\n",
    "    policy_stable = True\n",
    "    updated_Policy = []\n",
    "    for state in range(nS):\n",
    "        max_value = -math.inf\n",
    "        actions = []\n",
    "        quality_of_actions = []\n",
    "        new_policy = []\n",
    "        # Get the number of actions for the current state\n",
    "        number_of_actions = len(Policy[state])\n",
    "        # Loop through each action in the policy for the current state\n",
    "        for action in Policy[state]:\n",
    "            # Initialize the new_value and counter variables\n",
    "            new_value = 0\n",
    "            counter = 0\n",
    "            # Loop through each possible outcome for the current action in the current state\n",
    "            for a in env.env.P[state][action]:\n",
    "                # Initialize the reward variable\n",
    "                reward = 0\n",
    "                # Calculate the reward based on the outcome\n",
    "                if a[3] == True and a[1] == 15:\n",
    "                    reward = 1\n",
    "                elif a[3] == True:\n",
    "                    reward = -2\n",
    "                elif a[3] == False:\n",
    "                    reward = 0\n",
    "                # Calculate the expected value for the current action in the current state\n",
    "                new_value += a[0] * (reward + gamma * value_function[a[1]]) / number_of_actions\n",
    "            # Update the max_value variable if the new_value is greater than the current max_value\n",
    "            if max_value < new_value:\n",
    "                max_value = new_value\n",
    "            actions.append(action)\n",
    "            quality_of_actions.append(new_value)\n",
    "        for i in range(len(actions)):\n",
    "            # If the quality of the action is within a small margin of the max_value, add the action to the new_policy list\n",
    "            if max_value - 0.0001 < quality_of_actions[i]:\n",
    "                new_policy.append(actions[i])\n",
    "        # Add the new_policy to the updated_Policy list\n",
    "        updated_Policy.append(new_policy)\n",
    "    # Check if the updated policy is the same as the original policy\n",
    "    if updated_Policy == Policy:\n",
    "        policy_stable = False\n",
    "    return policy_stable, updated_Policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e56fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d88cbe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        result = self.env.step(action)\n",
    "        # modify the reward based on some co111ndition\n",
    "        if result[1] == 1:\n",
    "            result[1] = 1\n",
    "        elif result[1] == 0:\n",
    "            result[1] = -1\n",
    "        else:\n",
    "            result[1] = -2\n",
    "        return result\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "# # create the FrozenLake environment\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "\n",
    "# # create the reward wrapper\n",
    "# wrapped_env = CustomRewardWrapper(env)\n",
    "\n",
    "# # use the wrapped environment for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "045fcd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Operation_of_agent(Policy) :\n",
    "#     in this function we run the environmenet with given policy\n",
    "    env = gym.make(\"FrozenLake-v1\", is_slippery=False , render_mode='human' )\n",
    "    # run a random agent in the environment\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    action = random.choice(Policy[0])\n",
    "    while True :\n",
    "        result  = env.step(action)\n",
    "        action = random.choice(Policy[result[0]])\n",
    "        print(result[0])\n",
    "        print(result[2])\n",
    "        \n",
    "        if result[2] == True:\n",
    "            break\n",
    "\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef502b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def final_table(values):\n",
    "    matrix = np.zeros([4 , 4])\n",
    "    \n",
    "    for i in range (0, 4 ):\n",
    "        for j in range(0 ,4) :\n",
    "            matrix[i][j] = values[4*j + i]\n",
    "    # plot the matrix as an image with an appropriate colormap\n",
    "    plt.imshow(matrix.T, aspect='auto', cmap=\"bwr\")\n",
    "\n",
    "    # add the values\n",
    "    for (i, j), value in np.ndenumerate(matrix):\n",
    "        plt.text(i, j, \"%.3f\"%value, va='center', ha='center')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44fd67b",
   "metadata": {},
   "source": [
    "Checking the results :\\\n",
    "First we adjust the discount factor :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56c7222",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# discount factor = 1\n",
    "new_env = gym.make(\"FrozenLake-v1\", is_slippery=False )\n",
    "new_env = CustomRewardWrapper(env)\n",
    "state = new_env.reset()\n",
    "pi , po = policy_iteration(new_env.P , new_env.observation_space.n , new_env.action_space.n , new_env,gamma =0.9  )\n",
    "\n",
    "Operation_of_agent(po)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
